{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural search with doc2vec document embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows an alternative approach to neural search. We continue to use `gensim` and to work on the *Lee Background Corpus*, but we train document embeddings directly from the data.\n",
    "\n",
    "**Note:** This tutorial is an adapted version of the [Gensim Doc2Vec tutorial](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install gensim\n",
    "\n",
    "First, we need to make sure that `gensim` is installed. The installation may take a while.\n",
    "\n",
    "On a command-line Python version, install it the usual way: `pip install gensim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the corpus\n",
    "\n",
    "Let us load the corpus and display the beginnings of the first few news stories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "\n",
    "lee_bg_file = os.path.join(gensim.__path__[0], 'test', 'test_data', 'lee_background.cor')\n",
    "lee_bg_corpus = open(lee_bg_file, 'r')\n",
    "texts = lee_bg_corpus.readlines()\n",
    "lee_bg_corpus.close()\n",
    "for i in range(20):\n",
    "    print(f\"({i:>2})  {texts[i][:100]}...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the corpus to a list of TaggedDocuments\n",
    "\n",
    "In order to train document embeddings, the training data needs to follow a particular data structure. Therefore, we're going to preprocess each news story and save it as a `TaggedDocument`. The tag/label of the story is simply its numeric id (the number in parentheses above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for i, line in enumerate(texts):\n",
    "\ttext_without_stopwords = gensim.parsing.preprocessing.remove_stopwords(line)\n",
    "\ttokens = gensim.utils.simple_preprocess(text_without_stopwords)\n",
    "\tdoc = gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\tdocs.append(doc)\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the document vectors\n",
    "\n",
    "This time, we create the document vectors/embeddings from scratch using the `doc2vec` algorithm. With `gensim`, this is as simple as the three lines of code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document vectors with 50 dimensions and train for 100 epochs\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "model.build_vocab(docs)\n",
    "model.train(docs, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the document collection\n",
    "\n",
    "Now we can start writing queries and searching for the most relevant documents. For each query, we:\n",
    "\n",
    "- Process it in the same way as the document collection\n",
    "- Compute the document vector using the model trained above\n",
    "- Find the 5 most similar documents to the query\n",
    "- Display the similarity score, the document id and the beginning of the document text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Israel\",\n",
    "    \"investors capital profits financial services\",\n",
    "    \"militant terrorist killed\",\n",
    "    \"earthquake flood rain tsunami forest fire\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"Query:\", q)\n",
    "    q_without_stopwords = gensim.parsing.preprocessing.remove_stopwords(q)\n",
    "    q_tokens = gensim.utils.simple_preprocess(q_without_stopwords)\n",
    "    query_vector = model.infer_vector(q_tokens)\n",
    "    most_similar = model.dv.most_similar([query_vector], topn=5)\n",
    "    for doc_position, doc_score in most_similar:\n",
    "        print(f\"- {doc_score:.4f}  ({doc_position:>3})  {texts[doc_position][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these results better or worse than the ones with the averaged word vectors? Why do you think this is the case?\n",
    "\n",
    "What happens if you increase/decrease the vector size or the number of epochs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
