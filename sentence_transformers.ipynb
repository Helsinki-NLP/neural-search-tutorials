{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural search with BERT sentence embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training document embeddings, we use pretrained embeddings that work directly on sentence or document level.\n",
    "\n",
    "**Note:** This tutorial is an adapted version of the [semantic search example on SBERT](https://www.sbert.net/examples/applications/semantic-search/README.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install PyTorch and SentenceTransformers\n",
    "\n",
    "This installation may take a while. On a command-line Python version, install the packages the usual way, e.g. `pip install torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a document collection and create embeddings for them\n",
    "\n",
    "Let's use the examples from the SentenceBERT tutorial. Creating embeddings just involves two lines of code: one for defining (and downloading) the pre-trained model, and one for producing the embeddings properly speaking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "corpus = [\n",
    "    \"A man is eating food.\",\n",
    "    \"A man is eating a piece of bread.\",\n",
    "    \"The girl is carrying a baby.\",\n",
    "    \"A man is riding a horse.\",\n",
    "    \"A woman is playing violin.\",\n",
    "    \"Two men pushed carts through the woods.\",\n",
    "    \"A man is riding a white horse on an enclosed ground.\",\n",
    "    \"A monkey is playing drums.\",\n",
    "    \"A cheetah is running behind its prey.\",\n",
    "]\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
    "corpus_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen BERT model produces embeddings with 384 dimensions. As the corpus contains 9 examples, this corresponds to a 9 x 384 matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the document collection\n",
    "\n",
    "Now we can start writing queries and searching for the most relevant documents. For each query, we:\n",
    "- Produce its embedding using the same model as above\n",
    "- Find the 3 most similar documents in the corpus\n",
    "- Display the document text and the similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"A man is eating pasta.\",\n",
    "    \"Someone in a gorilla costume is playing a set of drums.\",\n",
    "    \"A cheetah chases prey on across a field.\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 3 scores\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    most_similar = torch.topk(cos_scores, k=3)\n",
    "    print(\"Query:\", query)\n",
    "    for score, idx in zip(most_similar[0], most_similar[1]):\n",
    "        print(f\"- {corpus[idx]} (Score: {score:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on a bigger document collection\n",
    "\n",
    "This seems to work fine. Let's go back to the Lee corpus that we used before and re-run the same pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "\n",
    "lee_bg_file = os.path.join(gensim.__path__[0], 'test', 'test_data', 'lee_background.cor')\n",
    "lee_bg_corpus = open(lee_bg_file, 'r')\n",
    "corpus = lee_bg_corpus.readlines()\n",
    "lee_bg_corpus.close()\n",
    "for i in range(20):\n",
    "    print(f\"({i:>2})  {corpus[i][:100]}...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're again using the `all-MiniLM-L6-v2` pretrained model, so we don't have to reload it. Producing the document embeddings will take some time here as the collection is a bit larger.\n",
    "\n",
    "We also add `.cpu()` to make saving and reloading easier. In practice, this notebook (and your Flask server) are not connected to a GPU, so everything is run on CPU anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True).cpu()\n",
    "corpus_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we get a 300 x 384 matrix because we have 300 examples in our corpus.\n",
    "\n",
    "**Note:** You can save the embeddings into a file like this:\n",
    "```\n",
    "import numpy as np\n",
    "np.savez_compressed(\"my_embeddings.npz\", data=corpus_embeddings)\n",
    "```\n",
    "\n",
    "and then load them again like this:\n",
    "```\n",
    "emb_file = np.load(\"my_embeddings.npz\")\n",
    "corpus_embeddings = torch.from_numpy(emb_file[\"data\"]).to(\"cpu\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the same queries as before and show the 5 most similar documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Israel\",\n",
    "    \"investors capital profits financial services\",\n",
    "    \"militant terrorist killed\",\n",
    "    \"earthquake flood rain tsunami forest fire\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"Query:\", q)\n",
    "    query_embedding = model.encode(q, convert_to_tensor=True).cpu()\n",
    "\n",
    "    # We use cosine similarity and torch.topk to find the highest 5 scores\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    most_similar = torch.topk(cos_scores, k=5)\n",
    "    for score, idx in zip(most_similar[0], most_similar[1]):\n",
    "        print(f\"- {score:.4f}  {corpus[idx][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! If you're interested in using sentence embeddings for your project, please have a look at the [semantic search page](https://www.sbert.net/examples/applications/semantic-search/README.html). In particular, check if other pretrained models are more suitable for your task and language than `all-MiniLM-L6-v2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
